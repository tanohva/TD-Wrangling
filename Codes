Etape 1 – Chargement & Nettoyage

Chargement des librairies

!pip install unidecode
import pathlib
import os
import sys
from IPython.display import display
import re
from unidecode import unidecode
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

1.Charger les fichiers suivants : directement de Kaggle
import kagglehub

# Download latest version
path = kagglehub.dataset_download("olistbr/brazilian-ecommerce")

print("Path to dataset files:", path)

for filename in os.listdir(path):
    print(filename)

# associer a nom court à chaque fichier csv
file_names={"orders":"olist_orders_dataset.csv",
    "items":"olist_order_items_dataset.csv",
    "reviews":"olist_order_reviews_dataset.csv",
    "payments":"olist_order_payments_dataset.csv",
    "customers":"olist_customers_dataset.csv",
    "geolocation":"olist_geolocation_dataset.csv",
    "products":"olist_products_dataset.csv",
    "sellers":"olist_sellers_dataset.csv",
    "category":"product_category_name_translation.csv"
    }
# créer les chemins des fichiers sous form de dict
path_datasets = {}
for k, v in file_names.items():
    path_dataset = {k :  os.path.join(path, v) }
    path_datasets.update(path_dataset)
    print(path_dataset)

#dfs est un doctionaire contient les 9 dataframes chargés
dfs = {}

for k, v in path_datasets.items():
    with open(v, "r") as f:
        df = pd.read_csv(f)
        dfs.update({k: df})

# Familiarisation
for x in dfs:
  print("----------------------------------------------------------------------------------------------------------------------------------------------------")
  print(x)
  print("-----------------------------------------------------------------------------------------------------------------------------------------------------")
  display(dfs[x].head())

for x in dfs:
  print("--------------------------------------------------------------------------------------------------------------------------------------------------")
  print(x)
  print("---------------------------------------------------------------------------------------------------------------------------------------------------")
  display(dfs[x].info())

# Descriptions
for x in dfs:
  print("----------------------------------------------------------------------------------------------------------------------------------------------------------")
  print(x)
  print("-----------------------------------------------------------------------------------------------------------------------------------------------------------")
  display(dfs[x].describe(include='all'))


TD Python – Data Wrangling avec le dataset Olist

lien DataSet : https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce?resource=download
Approche de préparation des données – Projet Olist
Groupes : 5, 6, 7, 8
Objectif principal

Prioriser la résolution des retards
Nettoyage

    Chargement des fichiers directement depuis le site Kaggle
    Familiarisation avec la structure des données (.head(), .info(), .describe())
    Conversion des types de données selon leur nature sémantique (dates, numériques, catégorielles, etc.)
    Normalisation des colonnes textuelles (ex. : noms de ville) pour supprimer les accents et harmoniser les formats
    Suppression des doublons
    Suppression des lignes vides
    Analyse des valeurs manquantes
    Traitement contextuel des valeurs manquantes (imputation, exclusion ou étiquetage)

Transformation

    Création de la colonne durée_livraison (date_livraison réelle - date_achat)
    Création de la colonne écart_livraison (date_estimée - date_livraison réelle)
    Création de la colonne booléenne livraison_à_temps
    Ajout d'une colonne montant_commande (produits + frais de livraison)
    Création de la colonne montant_frais_commande isolant les frais

Enrichissement

    Sélection des colonnes pertinentes pour l’analyse (features engineering)
    Jointure avec la table des clients pour récupérer les informations géographiques et les scores de satisfaction associés

Analyse exploratoire

    Répartition du score de satisfaction (review_score)
    Impact du retard de livraison sur la satisfaction
    Influence du montant de la commande sur la satisfaction
    Identification des villes avec le plus de commandes en retard
    Produits et catégories associés à des retards fréquents
    Durée moyenne de livraison par catégorie de produit (groupby + mean)

Etape 1 – Chargement & Nettoyage
Setup

Collecting unidecode
  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)
Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 235.8/235.8 kB 8.2 MB/s eta 0:00:00
Installing collected packages: unidecode
Successfully installed unidecode-1.4.0

1.Charger les fichiers suivants : directement de Kaggle

    list_customers_dataset.csv
    olist_sellers_dataset.csv
    olist_order_reviews_dataset.csv
    olist_order_items_dataset.csv
    olist_products_dataset.csv
    olist_geolocation_dataset.csv
    product_category_name_translation.csv
    olist_orders_dataset.csv
    olist_order_payments_dataset.csv

import kagglehub

# Download latest version

path = kagglehub.dataset_download("olistbr/brazilian-ecommerce")

print("Path to dataset files:", path)

for filename in os.listdir(path):
    print(filename)

#dfs est un doctionaire contient les 9 dataframes chargés

dfs = {}

for k, v in path_datasets.items():
    with open(v, "r") as f:
        df = pd.read_csv(f)
        dfs.update({k: df})

# Familiarisation

for x in dfs:
  print("----------------------------------------------------------------------------------------------------------------------------------------------------")
  print(x)
  print("-----------------------------------------------------------------------------------------------------------------------------------------------------")
  display(dfs[x].head())

for x in dfs:
  print("--------------------------------------------------------------------------------------------------------------------------------------------------")
  print(x)
  print("---------------------------------------------------------------------------------------------------------------------------------------------------")
  display(dfs[x].info())

#  Traitement des types de données
#  a) Classification sémantique des types de données
# liste de tout les colonne de type Date

date_columns=["order_purchase_timestamp","order_approved_at","order_delivered_carrier_date","order_delivered_customer_date","order_estimated_delivery_date",
              "shipping_limit_date",
              "review_creation_date","review_answer_timestamp"]

float_columns=["price","freight_value","payment_value","geolocation_lat","geolocation_lng","product_name_lenght","product_description_lenght","product_weight_g",
               "product_length_cm","product_height_cm","product_width_cm"]

int_columns=["review_score","payment_sequential","payment_installments","product_photos_qty"]

# b) Changement de type

for name, df in dfs.items():
    for col in df.columns:
        if col in date_columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')  # NaT en cas d'erreur
        elif col in int_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').astype("Int64")  # gère les NaN
        elif col in float_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').astype("float")  # gère les NaN

# c) Normalisation des colonne textuelle

def normalize_text(x):
    if isinstance(x, str): #vérifier pour ne pas toucher au Nan et number
        x = unidecode(x.lower()) #supprime les accents
        x = re.sub(r'\s+', ' ', x)  # remplace tous les espaces multiples par un seul
        return x.strip()
    return x

for x in dfs:
  df=dfs[x]
  for col in df.select_dtypes(include=["object", "string"]).columns: # normaliser juste les colonnes text
    df[col] = df[col].apply(normalize_text)

# 2.Supression des doublons :

for x in dfs:

  before = len(dfs[x])
  dfs[x].drop_duplicates(inplace=True)
  after = len(dfs[x])
  print(f"Lignes supprimées (doublons) table {x} : {before - after}")

# 3.Suppression des lignes vide (ligne avec toute les valeurs null)

for x in dfs:

  before = len(dfs[x])
  dfs[x].dropna(how='all',inplace=True)
  after = len(dfs[x])
  print(f"Lignes vides supprimés table {x} : {before - after}")

# a) Zoom sur les valeurs manquantes
for x in dfs:
    print("--------------------------------------------------------------------------------------------------------------------------------------------------")
    print(x)
    print("---------------------------------------------------------------------------------------------------------------------------------------------------")
    missing_percent = round(dfs[x].isna().sum() / len(dfs[x]) * 100, 2)
    print(missing_percent.astype(str) + "%")

#b) Vérification de la redondance selon les colonnes clés pour chaque table
   # orders-> (order_id,customer_id)
   # items-> (order_id,order_item_id,product_id,seller_id)
   # reviews-> (review_id,order_id)
   # payments-> (order_id,payment_sequential)
   # customers-> (customer_id, customer_unique_id)
   # geolocation-> (geolocation_zip_code_prefix,geolocation_lat,geolocation_lng)
   # products-> (product_id)
   # sellers-> (seller_id)
   # category-> (product_category_name)

keys_dict = {
    "orders": ["order_id", "customer_id"],
    "items": ["order_id", "order_item_id", "product_id", "seller_id"],
    "reviews": ["review_id", "order_id"],
    "payments": ["order_id", "payment_sequential"],
    "customers": ["customer_id", "customer_unique_id"],
    "geolocation": ['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng','geolocation_city', 'geolocation_state'],
    "products": ["product_id"],
    "sellers": ["seller_id"],
    "category": ["product_category_name"]
}

# affichage du nombre de lignes en double selon colonne_keys
for name, keys in keys_dict.items():
      count = dfs[name].duplicated(subset=keys).sum()
      print(f"{name}: {count} lignes dupliquées sur les clés {keys}")

# c) Traitement des valeurs manquante de la table orders selon le contexte métier

# récupérer toutes les lignes contenant au moins une valeur manquante (null)
df=dfs["orders"]
dt=df[df.isna().any(axis=1)]
dt

# 3 colonnes a traiter :
   # order_approved_at 0.16% : représente la date et l’heure à laquelle la commande a été approuvée.
   # order_delivered_carrier_date 1.79% : représente date d'expédition
   # order_delivered_customer_date 2.98% : represente date de livraison

# analysons par status de la commande
print("order_approved_at")
display(dt[dt["order_approved_at"].isna()].groupby("order_status").size().reset_index(name="nbr"))
print("--------------------------------------------------------------------------")
print("order_delivered_carrier_date")
display(dt[dt["order_delivered_carrier_date"].isna()].groupby("order_status").size().reset_index(name="nbr"))
print("--------------------------------------------------------------------------")
print("order_delivered_customer_date")
display(dt[dt["order_delivered_customer_date"].isna()].groupby("order_status").size().reset_index(name="nbr"))

#interprétation:
    #order_status="delivered" avec order_approved_at=NaT -> incohérent
    #order_status="delivered" avec order_delivered_carrier_date = NaT -> incohérent.
    #order_status="delivered" avec order_delivered_customer_date=Nat -> incohérent

# 1er Cas: order_status="delivered" avec order_approved_at=NaT
#1. order_status="delivered" avec order_approved_at=NaT -> incohérent
df=dfs["orders"]
dt=df[df.isna().any(axis=1)]
dh=dt[dt["order_approved_at"].isna() & dt["order_status"].str.contains("delivered")]
dh

# Nous vérifions d'abord si le paiement a été effectué. Si c'est le cas, nous imputons les dates ; sinon, nous corrigeons le champ order_status.
jointure = dh.merge(dfs["payments"], on="order_id", how="left")
jointure

# Paiement confirmé pour toutes les lignes : imputation de la date de validation par date_commande + délai moyen de validation.
duree_moyenne = (dt["order_approved_at"] - dt["order_purchase_timestamp"]).dropna().mean()
duree_moyenne = pd.to_timedelta(round(duree_moyenne.total_seconds()), unit='s') # arrondir en seconde
dfs["orders"].loc[dh.index, "order_approved_at"] = dfs["orders"]["order_purchase_timestamp"] + duree_moyenne

#verification des valeurs aprés imputations
dfs["orders"].loc[dh.index] #dh contient les ligne avec val NaN

#2ème Cas: order_status="delivered" avec order_delivered_carrier_date = NaT
# 2. order_status="delivered" avec order_delivered_carrier_date = NaT -> incohérent.
df=dfs["orders"]
dt=df[df.isna().any(axis=1)]
dh=dt[dt["order_delivered_carrier_date"].isna() & dt["order_status"].str.contains("delivered")]
dh

#Vérification du paiement
jointure = dh.merge(dfs["payments"], on="order_id", how="left")
jointure

# Paiement confirmé pour toutes les lignes : Imputation basée sur la date de validation et la durée moyenne écoulée entre la date de validation et la date d’expédition.
duree_moyenne = (dt["order_delivered_carrier_date"] - dt["order_approved_at"]).dropna().mean()
duree_moyenne = pd.to_timedelta(round(duree_moyenne.total_seconds()), unit='s') # arrondir en seconde
dfs["orders"].loc[dh.index, "order_delivered_carrier_date"] = dfs["orders"]["order_approved_at"] + duree_moyenne

#verification des valeurs aprés imputations
dfs["orders"].loc[dh.index] #dh contient les ligne avec val NaN

# 3 eme Cas: order_status="delivered" avec order_delivered_customer_date=Nat
#1. order_status="delivered" avec order_approved_at=NaT -> incohérent
df=dfs["orders"]
dt=df[df.isna().any(axis=1)]
dh=dt[dt["order_delivered_customer_date"].isna() & dt["order_status"].str.contains("delivered")]
dh

#Verification des paiements
jointure = dh.merge(dfs["payments"], on="order_id", how="left")
jointure

# paiment effectué : Imputation de order_delivered_customer_date par order_delivered_carrier_date + délai moyen transporteur → client.
duree_moyenne = (dfs["orders"]["order_delivered_customer_date"] - dfs["orders"]["order_delivered_carrier_date"]).dropna().mean()
duree_moyenne = pd.to_timedelta(round(duree_moyenne.total_seconds()), unit='s') # arrondir en seconde
dfs["orders"].loc[dh.index, "order_delivered_customer_date"] = dfs["orders"]["order_delivered_carrier_date"] + duree_moyenne

#verification des valeurs aprés imputations
dfs["orders"].loc[dh.index]

# d) Vérification Globale
# revérification globale
df=dfs["orders"]
dt=df[df.isna().any(axis=1)]

print("order_approved_at")
display(dt[dt["order_approved_at"].isna()].groupby("order_status").size())
print("--------------------------------------------------------------------------")
print("order_delivered_carrier_date")
display(dt[dt["order_delivered_carrier_date"].isna()].groupby("order_status").size())
print("--------------------------------------------------------------------------")
print("order_delivered_customer_date")
display(dt[dt["order_delivered_customer_date"].isna()].groupby("order_status").size())

#  Valeurs nulles de paiement
df=dfs["payments"]
dh=df[df["payment_value"]==0]
dh

dh.columns

# Un paiement avec valeur=0-> vérifier l'état de la commande
jointure = dh.merge(dfs["orders"], on="order_id", how="left")
jointure.groupby(["order_status","payment_type"]).size()

# Etape 2 – Transformation des données

#  4.Calcule la durée de livraison pour chaque commande : order_delivered_customer_date - order_purchase_timestamp

dfs["orders"]["duree_livraison"]=dfs["orders"]["order_delivered_customer_date"]-dfs["orders"]["order_purchase_timestamp"]
dfs["orders"].head()

# 5.Calcule l’écart entre la date estimée et la date réelle de livraison.

# Crée une variable binaire on_time_delivery qui vaut True si la commande a été livrée à temps ou en avance.
# onconcidére que une commande si elle est livré le meme jour estimé elle est a temps

dfs["orders"]["ecart_livraison"] = dfs["orders"].apply(
    lambda row: (row["order_estimated_delivery_date"] - row["order_delivered_customer_date"]).days
    if pd.notnull(row["order_delivered_customer_date"]) else pd.NA,
    axis=1)

#True si elle est livré a temps, false si en retard, Nat si n'est pas livré
dfs["orders"]["livre_a_temps"] = dfs["orders"].apply(
    lambda row: row["order_delivered_customer_date"] <= row["order_estimated_delivery_date"]
    if pd.notnull(row["order_delivered_customer_date"]) else pd.NA,
    axis=1
)

# 6.Calcule le montant total payé par commande (en fusionnant les paiements et les items).

# le montant total d'une commande est la somme des montant des items + les frais
# calculons le montant total de chaque commande
df_mnt = dfs["items"].groupby("order_id")[["price", "freight_value"]].sum()
df_mnt["montant_cmd"] = (df_mnt["price"] + df_mnt["freight_value"])
df_mnt = df_mnt[["montant_cmd"]].reset_index()
df_mnt

# insérer le montant total de la commande dans la table orders avec une jointure a gauche
dfs["orders"].drop(columns=["montant_cmd"], inplace=True, errors="ignore")
dfs["orders"] = dfs["orders"].merge(df_mnt, on="order_id", how="left")
dfs["orders"].head()

# Montant reellement payé pour chaque CMD
df_paiements = dfs["payments"].groupby("order_id")["payment_value"].sum().reset_index(name="montant_paye")
dfs["orders"].drop(columns=["montant_paye"], inplace=True, errors="ignore")
dfs["orders"] = dfs["orders"].merge(df_paiements, on="order_id", how="left")
dfs["orders"].head()

# 7.Calcule le frais de port moyen par commande.
df_frais = dfs["items"].groupby("order_id")["freight_value"].sum().reset_index(name="frais_port")
dfs["orders"].drop(columns=["frais_port"], inplace=True, errors="ignore")
dfs["orders"] = dfs["orders"].merge(df_frais, on="order_id", how="left")
dfs["orders"].head()

frais_moyen_cmd=dfs["orders"]["frais_port"].mean()
print("frais moyen de port par commande est :",round(frais_moyen_cmd,2),"$")

# Etape 3 – Fusion et enrichissement
# 8.Crée une table finale contenant, pour chaque commande :

 #ID client
 # Date de commande
 # Délai de livraison
 # Estimation vs réel
 # Score de satisfaction (review_score)
 # Total payé
 # Livraison à l’heure (oui/non)

df_score=dfs["reviews"].groupby("order_id")["review_score"].mean().round() # une commande peut avoir plusieurs review, on prens la moyenne de review_score
#insertion de score_view dans orders avec left join
dfs["orders"].drop(columns=["review_score"], inplace=True, errors="ignore")
dfs["orders"] = dfs["orders"].merge(df_score, on="order_id", how="left")
dfs["orders"].head()

# Table Commande
df_cmd=dfs["orders"][["order_id","customer_id","order_purchase_timestamp","duree_livraison","ecart_livraison","review_score","montant_cmd","montant_paye","livre_a_temps"]]
df_cmd.head()

#  9.Integration données client
# pour eviter la redondance dans la table customers si un client a plusieurs code zip nous prenons directement le premier code seulement
df_zip=dfs["customers"].groupby("customer_id")["customer_zip_code_prefix"].first().reset_index(name="code_zip")
# insertion du code_zip dans la nouvelle table df_cmd
df_cmd.drop(columns=["code_zip"], inplace=True, errors="ignore")
df_cmd = df_cmd.merge(df_zip, on="customer_id", how="left")
df_cmd.head()

# Etape 4 - Analyse Exploratoire

# 10.Quelle est la répartition des scores de satisfaction (1 à 5)

plt.figure(figsize=(8, 5))  # Taille du graphique

sns.histplot(df_cmd["review_score"], bins=5, kde=False, edgecolor="black")

plt.title("Répartition des scores de satisfaction (1 à 5)", fontsize=14)
plt.xlabel("Score de satisfaction", fontsize=12)
plt.ylabel("Nombre de commandes", fontsize=12)

plt.xticks(ticks=[1, 2, 3, 4, 5], fontsize=10)

plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

# 11.Le délai de livraison a-t-il un impact sur la satisfaction ?

df_cmd.head()
df_cmd.dtypes

# Ajouter la colonne de durée en jours
df_cmd["duree_livraison_jours"] = dfs["orders"]["duree_livraison"].dt.days

# Affichage boxplot
plt.figure(figsize=(8, 5))
sns.boxplot(data=df_cmd, x="review_score", y="duree_livraison_jours", hue="review_score")
plt.ylim(0, 50)
plt.title("Distribution de la durée de livraison par score de satisfaction")
plt.xlabel("Score de satisfaction")
plt.ylabel("Durée de livraison (jours)")
plt.tight_layout()
plt.show()

#  Affiche la moyenne de review_score selon que la commande a été livrée à temps ou non.
dh=df_cmd.groupby("livre_a_temps")["review_score"].mean().reset_index(name="score_satis_moyen")
dh

plt.figure(figsize=(6, 4))

sns.barplot(data=dh, x="livre_a_temps", y="score_satis_moyen",hue="livre_a_temps", palette=["#4caf50", "#f44336"])

plt.title("Impact des délais de livraison sur la satisfaction", fontsize=14)
plt.xlabel("Livraison à temps", fontsize=12)
plt.ylabel("Score moyen de satisfaction", fontsize=12)

# Modifier les labels de l'axe X (booléen → lisible)
plt.xticks(ticks=[0, 1], labels=["En retard", "À temps"])

plt.legend=None

plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()


# 12.Quel est le montant moyen des commandes selon le score de satisfaction ?
df_cmd.groupby("review_score")["montant_cmd"].mean().round(2)

#  13.Quelles sont les 5 villes avec le plus de commandes livrées en retard ?

# liste des (zip,city)
df_city=dfs["geolocation"][["geolocation_zip_code_prefix", "geolocation_city"]].drop_duplicates()
# df_city.head()
#recupération des noms des villes
df_cmd=df_cmd.merge(df_city,left_on="code_zip",right_on="geolocation_zip_code_prefix",how="left")
df_cmd.head()

#filter les 5 villes avec plus de cmd en retard
df_retard = df_cmd.dropna(subset=["livre_a_temps"])
df_retard = df_retard[df_retard["livre_a_temps"] == False]
dh=df_retard.groupby("geolocation_city").size().sort_values(ascending=False).head(5).reset_index(name="nbr")
dh

#representation graphique des 5 villes avec plus de CMD en retard

# Barplot horizontal
plt.figure(figsize=(7, 5))
sns.barplot(
    data=dh,
    x="geolocation_city",
    y="nbr",
    palette="viridis",hue="geolocation_city",
    orient="v"
)
plt.xlabel("Ville")
plt.ylabel("Nbr de CMD en retard")
plt.title("5 Top ville avec plus de commandes en retard")
plt.tight_layout()
plt.show()

# 14.Quels sont les produits qui génèrent le plus de retards ?
#liste des categorie des produit commandés
dt1=dfs["products"][["product_id","product_category_name"]].dropna()
dt2=dfs["items"][["order_id","product_id"]].dropna()

df_product=dt2.merge(dt1,how="left",on="product_id").dropna()
df_product.head()

#liste des commandes en retard

dh=df_retard.merge(df_product,how="inner",on="order_id").groupby(["product_category_name"]).size().sort_values(ascending=False).head(10).reset_index(name="nbr")
dh

# representation graphique
# Barplot horizontal
plt.figure(figsize=(10, 5))
sns.barplot(
    data=dh,
    y="product_category_name",
    x="nbr",
    palette="viridis",hue="product_category_name",
)
plt.xlabel("Nombre de CMD")
plt.ylabel("Catégorie produit")
plt.title("Top 10 catégories des produits qui generent plus de retard")
plt.tight_layout()
plt.show()

# 15.Quel est le nombre moyen de jours de retard par catégorie de produit (product_category_name) ?

dh=df_retard.merge(df_product,how="inner",on="order_id").groupby(["product_category_name"])["duree_livraison_jours"].mean().round()
dh.head(10)

df = dh.head(10).reset_index()

# Barplot horizontal
plt.figure(figsize=(10, 8))
sns.barplot(
    data=df,
    x="duree_livraison_jours",
    y="product_category_name",
    palette="viridis",hue="product_category_name"
)
plt.xlabel("Durée de livraison (jours)")
plt.ylabel("Catégorie de produit")
plt.title("Durée moyenne de livraison par catégorie (en jours)")
plt.tight_layout()
plt.show()

